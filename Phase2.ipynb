{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0376f99",
   "metadata": {},
   "source": [
    "# Phase 2 â€” Data Summarization and Preprocessing\n",
    "\n",
    "This notebook follows the Phase#2 instructions and is ready to run. Place your raw dataset file named **`Raw_dataset.csv`** in the same directory as this notebook or update the `DATA_PATH` variable below to point to the correct file in your repo.\n",
    "\n",
    "Sections included:\n",
    "\n",
    "1. Load dataset\n",
    "2. Data overview and statistical summaries (five-number summary, etc.)\n",
    "3. Missing values analysis\n",
    "4. Variable distributions and plots (histograms, boxplots, bar plots)\n",
    "5. Class label distribution plot\n",
    "6. Outlier detection\n",
    "7. Preprocessing (missing value treatment, encoding, scaling, feature selection)\n",
    "8. Save preprocessed dataset\n",
    "\n",
    "Notes:\n",
    "- Do not modify the original file; the processed output will be saved as `Preprocessed_dataset.csv`.\n",
    "- Run each cell in order. If your dataset file has a different name or is inside a folder, change `DATA_PATH` accordingly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326268b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Imports and load dataset\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', 200)\n",
    "pd.set_option('display.width', 200)\n",
    "\n",
    "# Path to your raw dataset (change if needed)\n",
    "DATA_PATH = 'Raw_dataset.csv'  # <-- change if your file name/path differs\n",
    "\n",
    "# Try loading dataset\n",
    "if not os.path.exists(DATA_PATH):\n",
    "    print(f\"Warning: {DATA_PATH} not found in notebook directory.\\nPlease put your Raw_dataset.csv next to this notebook or update DATA_PATH.\")\n",
    "else:\n",
    "    df = pd.read_csv(DATA_PATH)\n",
    "    print('Loaded dataset with shape:', df.shape)\n",
    "    display(df.head(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32823de2",
   "metadata": {},
   "source": [
    "## 2) Data overview and statistical summaries\n",
    "\n",
    "Compute number of instances, attributes, data types, and provide five-number summary for numeric attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8b81b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of instances and attributes, datatypes\n",
    "try:\n",
    "    print('Number of records (instances):', df.shape[0])\n",
    "    print('Number of attributes (columns):', df.shape[1])\n",
    "    print('\\nColumn datatypes:')\n",
    "    display(df.dtypes)\n",
    "except NameError:\n",
    "    print('Dataset not loaded. Run the load cell and ensure DATA_PATH is correct.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e30f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Five-number summary (min, Q1, median, Q3, max) for numeric columns\n",
    "try:\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    print('Numeric columns detected:', numeric_cols)\n",
    "    display(df[numeric_cols].describe().T[['min','25%','50%','75%','max']].rename(columns={'25%':'Q1','50%':'median','75%':'Q3'}))\n",
    "except NameError:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc40baa",
   "metadata": {},
   "source": [
    "## 3) Missing values analysis\n",
    "\n",
    "Show total and percent of missing values per column and a simple strategy recommendation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70823f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values table\n",
    "try:\n",
    "    miss = df.isnull().sum().to_frame('missing_count')\n",
    "    miss['missing_pct'] = miss['missing_count'] / df.shape[0] * 100\n",
    "    display(miss.sort_values('missing_pct', ascending=False))\n",
    "except NameError:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649376de",
   "metadata": {},
   "source": [
    "## 4) Variable distributions & plots\n",
    "\n",
    "At least 3 different plotting types: histogram (numeric), boxplot (numeric/outliers), bar plot (categorical)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4abe0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper plotting function - run this cell to create multiple plots\n",
    "try:\n",
    "    import math\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    cat_cols = df.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "    print('Numeric columns:', numeric_cols)\n",
    "    print('Categorical columns:', cat_cols)\n",
    "\n",
    "    # Histograms for up to 6 numeric columns\n",
    "    n = min(6, len(numeric_cols))\n",
    "    if n>0:\n",
    "        cols = numeric_cols[:n]\n",
    "        df[cols].hist(bins=15, figsize=(12, 3*n))\n",
    "        plt.suptitle('Histograms (first numeric columns)')\n",
    "        plt.show()\n",
    "\n",
    "    # Boxplots for numeric columns (first 6)\n",
    "    if n>0:\n",
    "        fig, axes = plt.subplots(n, 1, figsize=(10, 4*n))\n",
    "        if n==1:\n",
    "            axes = [axes]\n",
    "        for ax, col in zip(axes, cols):\n",
    "            df.boxplot(column=col, ax=ax)\n",
    "            ax.set_title(f'Boxplot - {col}')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # Bar plots for top categorical columns (show value counts)\n",
    "    m = min(4, len(cat_cols))\n",
    "    if m>0:\n",
    "        for col in cat_cols[:m]:\n",
    "            vc = df[col].value_counts(dropna=False).nlargest(10)\n",
    "            vc.plot(kind='bar', figsize=(8,4))\n",
    "            plt.title(f'Value counts for {col} (top 10)')\n",
    "            plt.ylabel('Count')\n",
    "            plt.show()\n",
    "    else:\n",
    "        print('No categorical columns detected for bar plots.')\n",
    "except NameError:\n",
    "    print('Dataset not loaded. Run the load cell first.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8059709b",
   "metadata": {},
   "source": [
    "## 5) Class label distribution\n",
    "\n",
    "If you have a class/target column, set TARGET_COL variable and visualize its distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728a821a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class label distribution - update TARGET_COL if necessary\n",
    "TARGET_COL = None  # <-- e.g. 'target' or 'class'. Set to column name if available.\n",
    "\n",
    "try:\n",
    "    if TARGET_COL is None:\n",
    "        # try to guess a likely target column (common names)\n",
    "        for guess in ['target','class','label','y','Outcome','outcome','grade']:\n",
    "            if guess in df.columns:\n",
    "                TARGET_COL = guess\n",
    "                print('Auto-detected target column:', TARGET_COL)\n",
    "                break\n",
    "\n",
    "    if TARGET_COL is not None and TARGET_COL in df.columns:\n",
    "        vc = df[TARGET_COL].value_counts(dropna=False)\n",
    "        display(vc.to_frame('count'))\n",
    "        vc.plot(kind='bar', figsize=(6,4))\n",
    "        plt.title(f'Class distribution: {TARGET_COL}')\n",
    "        plt.ylabel('Count')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print('No target column specified or detected. Set TARGET_COL variable to your class column name.')\n",
    "except NameError:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a7167f",
   "metadata": {},
   "source": [
    "## 6) Outlier detection (IQR method)\n",
    "\n",
    "Detect outliers using interquartile range for numeric features and report counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045ee3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    outlier_summary = []\n",
    "    for col in numeric_cols:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower = Q1 - 1.5*IQR\n",
    "        upper = Q3 + 1.5*IQR\n",
    "        out_count = ((df[col] < lower) | (df[col] > upper)).sum()\n",
    "        outlier_summary.append((col, int(out_count)))\n",
    "    out_df = pd.DataFrame(outlier_summary, columns=['column','outlier_count']).sort_values('outlier_count', ascending=False)\n",
    "    display(out_df)\n",
    "except NameError:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf46032",
   "metadata": {},
   "source": [
    "## 7) Preprocessing\n",
    "\n",
    "Apply at least three preprocessing tasks (not just removing attributes or splitting dataset). Examples included below: missing value imputation, categorical encoding, scaling/normalization, and feature selection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22eb0463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing pipeline (example). Modify as needed for your dataset.\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# Make a copy to avoid modifying original dataframe\n",
    "try:\n",
    "    df_proc = df.copy()\n",
    "except NameError:\n",
    "    df_proc = None\n",
    "    print('Dataset not loaded.')\n",
    "\n",
    "# 1) Missing value handling recommendations & example (median for numeric, most_frequent for categorical)\n",
    "try:\n",
    "    num_cols = df_proc.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    cat_cols = df_proc.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "    print('Numeric columns:', num_cols)\n",
    "    print('Categorical columns:', cat_cols)\n",
    "except Exception as e:\n",
    "    print('Error detecting columns:', e)\n",
    "\n",
    "# Example transformers (do not run if dataset not loaded)\n",
    "def build_preprocessor(use_scaler='standard', encode_type='onehot'):\n",
    "    # numeric pipeline: impute + scaler\n",
    "    numeric_pipeline = [('imputer', SimpleImputer(strategy='median'))]\n",
    "    if use_scaler == 'standard':\n",
    "        numeric_pipeline.append(('scaler', StandardScaler()))\n",
    "    elif use_scaler == 'minmax':\n",
    "        numeric_pipeline.append(('scaler', MinMaxScaler()))\n",
    "    # categorical pipeline: impute + encode\n",
    "    cat_pipeline = [('imputer', SimpleImputer(strategy='most_frequent'))]\n",
    "    if encode_type == 'onehot':\n",
    "        cat_pipeline.append(('encoder', OneHotEncoder(handle_unknown='ignore', sparse=False)))\n",
    "    else:\n",
    "        cat_pipeline.append(('encoder', OrdinalEncoder()))\n",
    "    transformers = []\n",
    "    if len(num_cols) > 0:\n",
    "        transformers.append(('num', Pipeline(steps=numeric_pipeline), num_cols))\n",
    "    if len(cat_cols) > 0:\n",
    "        transformers.append(('cat', Pipeline(steps=cat_pipeline), cat_cols))\n",
    "    col_transformer = ColumnTransformer(transformers=transformers, remainder='drop', sparse_threshold=0)\n",
    "    return col_transformer\n",
    "\n",
    "# Note: Pipeline imported below to avoid import errors if scikit-learn is not present\n",
    "try:\n",
    "    from sklearn.pipeline import Pipeline\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "print('\\nPreprocessing instructions:')\n",
    "print('- Build a ColumnTransformer with numeric imputation + scaling and categorical imputation + encoding.')\n",
    "print('- Optionally apply feature selection with SelectKBest (if you have a labeled TARGET_COL).')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef58f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: If you have a target, run feature selection and save preprocessed dataset\n",
    "try:\n",
    "    if 'df_proc' in globals() and df_proc is not None:\n",
    "        # Simple example: drop columns with > 60% missing values, then impute remaining\n",
    "        thresh = 0.6\n",
    "        to_drop = miss[miss['missing_pct'] > thresh*100].index.tolist()\n",
    "        print('Dropping columns with > 60% missing:', to_drop)\n",
    "        df_proc = df_proc.drop(columns=to_drop, errors='ignore')\n",
    "\n",
    "        # Impute numeric and categorical using the simple strategies from above\n",
    "        from sklearn.impute import SimpleImputer\n",
    "        for c in df_proc.select_dtypes(include=[np.number]).columns:\n",
    "            if df_proc[c].isnull().any():\n",
    "                df_proc[c] = SimpleImputer(strategy='median').fit_transform(df_proc[[c]])\n",
    "        for c in df_proc.select_dtypes(exclude=[np.number]).columns:\n",
    "            if df_proc[c].isnull().any():\n",
    "                df_proc[c] = SimpleImputer(strategy='most_frequent').fit_transform(df_proc[[c]]).ravel()\n",
    "\n",
    "        print('After imputation, missing values per column:')\n",
    "        display(df_proc.isnull().sum().to_frame('missing_count'))\n",
    "\n",
    "        # Save a snapshot of raw (already available) and preprocessed dataset\n",
    "        out_raw = 'Snapshot_raw_first5rows.csv'\n",
    "        df.head(5).to_csv(out_raw, index=False)\n",
    "        out_pre = 'Preprocessed_dataset.csv'\n",
    "        df_proc.to_csv(out_pre, index=False)\n",
    "        print('\\nSaved snapshot of raw data to', out_raw)\n",
    "        print('Saved preprocessed dataset to', out_pre)\n",
    "    else:\n",
    "        print('No dataset to preprocess. Load dataset first.')\n",
    "except NameError:\n",
    "    print('Dataset not loaded or error in preprocessing.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a660ef43",
   "metadata": {},
   "source": [
    "## Checklist / Deliverables\n",
    "\n",
    "- [ ] Notebook `Phase2.ipynb` with analysis and preprocessing steps.\n",
    "- [ ] Plots showing variable distributions (histograms, boxplots, bar plots).\n",
    "- [ ] Missing values analysis and handling.\n",
    "- [ ] Statistical summaries (five-number summary for numeric attributes).\n",
    "- [ ] Class label distribution plot.\n",
    "- [ ] Preprocessed dataset exported as `Preprocessed_dataset.csv`.\n",
    "\n",
    "### How to run\n",
    "1. Place `Raw_dataset.csv` in the same folder as this notebook or change `DATA_PATH`.\n",
    "2. Run the notebook top-to-bottom.\n",
    "3. Review outputs, modify preprocessing choices, and re-run cells as needed.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
